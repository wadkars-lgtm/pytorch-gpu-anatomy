{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3e1378bbc60e29d8",
      "metadata": {},
      "source": ""
    },
    {
      "cell_type": "markdown",
      "id": "ca06f523908dcfaa",
      "metadata": {},
      "source": "Adapted from Karpathy - Let's build GPT from scratch, in code, spelled out ([Video](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=797s))"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5fe9500f70dcd1be",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:34:33.972873Z",
          "start_time": "2025-12-27T19:34:33.969873Z"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000 # how many training iterations to run for\n",
        "eval_interval = 500 # how often to evaluate the model on the validation set\n",
        "learning_rate = 3e-4 # what learning rate to use\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200 # how many batches to use for evaluation\n",
        "n_embed = 384 # dimensionality of the token embeddings\n",
        "n_head = 6 # how many heads to use in the multi-head attention\n",
        "n_layer = 6 # how many layers to use in the transformer\n",
        "dropout = 0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1efd95a42d3a28b6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:34:36.552007Z",
          "start_time": "2025-12-27T19:34:36.016069Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\wadka\\Documents\\GitHub\\pytorch-gpu-anatomy\\notebooks\n",
            "Downloading Tiny Shakespeare...\n",
            "Saved to data\\shakespeare_char\\input.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "print(os.getcwd())\n",
        "# Where to save it (match nanoGPT structure if you want)\n",
        "out_dir = os.path.join(\"data\", \"shakespeare_char\")\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "out_path = os.path.join(out_dir, \"input.txt\")\n",
        "\n",
        "print(\"Downloading Tiny Shakespeare...\")\n",
        "urllib.request.urlretrieve(url, out_path)\n",
        "print(f\"Saved to {out_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5e688f35380a510f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:34:38.184478Z",
          "start_time": "2025-12-27T19:34:38.157429Z"
        }
      },
      "outputs": [],
      "source": [
        "with open(f\"{out_path}\",encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a3475bbc744a94b3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:34:40.245880Z",
          "start_time": "2025-12-27T19:34:40.242968Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \",len(text))\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a84b2f07cf3bd7a4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:34:41.847237Z",
          "start_time": "2025-12-27T19:34:41.839996Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all the unique characters: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(set(text))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", chars)\n",
        "print(''.join(chars))\n",
        "print(\"vocab size:\", vocab_size)\n",
        "# %%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "bd0e3e3b4d3306d0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:34:43.403129Z",
          "start_time": "2025-12-27T19:34:43.399130Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = dict(enumerate(chars))\n",
        "\n",
        "def encode(mystr):\n",
        "    return [stoi[c] for c in mystr]\n",
        "\n",
        "def decode(tokens):\n",
        "    return ''.join([itos[i] for i in tokens])\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a586eb288c6a23da",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:34:45.236164Z",
          "start_time": "2025-12-27T19:34:45.166980Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])\n",
        "# %%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "cb78a1decc7e30e6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:34:47.803545Z",
          "start_time": "2025-12-27T19:34:47.801238Z"
        }
      },
      "outputs": [],
      "source": [
        "#Separate the dataset into train and test\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "# %%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e9e0df3b04b2720d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:34:54.028241Z",
          "start_time": "2025-12-27T19:34:54.022137Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([64, 256])\n",
            "tensor([[ 1, 41, 46,  ..., 59, 50, 42],\n",
            "        [56, 58, 59,  ..., 46, 39, 58],\n",
            "        [53,  1, 46,  ..., 56, 59, 52],\n",
            "        ...,\n",
            "        [45, 53, 53,  ..., 56, 43,  1],\n",
            "        [52, 42,  1,  ..., 58, 46, 59],\n",
            "        [39,  1, 50,  ..., 39,  1, 52]])\n",
            "targets:\n",
            "torch.Size([64, 256])\n",
            "tensor([[41, 46, 39,  ..., 50, 42,  1],\n",
            "        [58, 59, 56,  ..., 39, 58,  6],\n",
            "        [ 1, 46, 47,  ..., 59, 52, 41],\n",
            "        ...,\n",
            "        [53, 53, 42,  ..., 43,  1, 63],\n",
            "        [42,  1, 39,  ..., 46, 59, 52],\n",
            "        [ 1, 50, 53,  ...,  1, 52, 47]])\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "def get_batch(split):\n",
        "    #generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    #Generate starting indexes for each sequence\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    #Take consecutive blocks of data starting from ix\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    #Take consecutive blocks of data starting from ix+1\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "40b71aefff64dfc6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:34:56.774218Z",
          "start_time": "2025-12-27T19:34:56.742978Z"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x203ce698d10>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d8f50a7fa8ed1b68",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:49:00.978029Z",
          "start_time": "2025-12-27T19:49:00.972548Z"
        }
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    One causal self-attention head.\n",
        "\n",
        "    Input:\n",
        "      x: (B, T, n_embed)  # B=batch size, T=sequence length, n_embed=model width\n",
        "\n",
        "    Output:\n",
        "      out: (B, T, head_size)\n",
        "\n",
        "    What it computes (conceptually):\n",
        "      For each position t, produce a weighted sum of \"value\" vectors from positions <= t.\n",
        "      The weights come from similarity between the current token's \"query\" and all tokens' \"keys\",\n",
        "      with a causal mask that prevents looking into the future.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, head_size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # Project the model embedding (n_embed) into key/query/value subspaces (head_size).\n",
        "        # These are learned linear maps shared across all tokens and time steps.\n",
        "        #\n",
        "        # key(x)[b,t]   = K vector used as \"address\" for token at time t\n",
        "        # query(x)[b,t] = Q vector used to ask \"what should I attend to from the past?\"\n",
        "        # value(x)[b,t] = V vector that contains the information we will mix/aggregate\n",
        "        #\n",
        "        # Shapes:\n",
        "        #   x: (B, T, n_embed)\n",
        "        #   key/query/value outputs: (B, T, head_size)\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "\n",
        "        # Precompute a lower-triangular causal mask of shape (block_size, block_size).\n",
        "        #\n",
        "        # tril[i, j] = 1 if j <= i else 0\n",
        "        #\n",
        "        # We register it as a buffer (not a Parameter) because:\n",
        "        #   - it is not learned\n",
        "        #   - it should move with the module to GPU/CPU (m.to(device))\n",
        "        #   - it should be saved/loaded with the state_dict\n",
        "        self.register_buffer(\n",
        "            \"tril\",\n",
        "            torch.tril(torch.ones(block_size, block_size))\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute causal self-attention for one head.\n",
        "\n",
        "        Args:\n",
        "          x: (B, T, n_embed)\n",
        "\n",
        "        Returns:\n",
        "          out: (B, T, head_size)\n",
        "        \"\"\"\n",
        "        # Unpack shapes.\n",
        "        # C here is n_embed (model width). Don't confuse it with head_size.\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Compute Keys and Queries for all tokens in the sequence.\n",
        "        # k[b, t, :] and q[b, t, :] are vectors in R^(head_size).\n",
        "        k = self.key(x)     # (B, T, head_size)\n",
        "        q = self.query(x)   # (B, T, head_size)\n",
        "\n",
        "        # Compute raw attention scores (\"affinities\") between each query and each key.\n",
        "        #\n",
        "        # For each batch b:\n",
        "        #   q[b] is (T, head_size)\n",
        "        #   k[b].transpose(-2, -1) is (head_size, T)\n",
        "        #   q[b] @ k[b]^T gives (T, T), where entry (t, i) is dot(q_t, k_i)\n",
        "        #\n",
        "        # After broadcasting over batch:\n",
        "        #   (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "        #\n",
        "        # Scaling by 1/sqrt(head_size) stabilizes softmax:\n",
        "        # dot products grow with dimension; without scaling softmax can saturate (very peaky),\n",
        "        # hurting gradients and training stability.\n",
        "        wei = q @ k.transpose(-2, -1) * (k.size(-1) ** -0.5)   # (B, T, T)\n",
        "\n",
        "        # Apply causal mask: disallow attention to future positions.\n",
        "        #\n",
        "        # self.tril[:T, :T] is a (T, T) matrix with 1s in the lower triangle.\n",
        "        # Positions where mask == 0 correspond to \"future\" tokens (i < j) for a given row.\n",
        "        #\n",
        "        # masked_fill sets those illegal positions to -inf so that after softmax they become 0.\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
        "\n",
        "        # Convert scores to probabilities along the last dimension (over \"keys\"/time positions).\n",
        "        #\n",
        "        # For each (b, t), wei[b, t, :] becomes a distribution over i in [0..T-1],\n",
        "        # and due to masking, probability mass is only on i <= t.\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # Compute Values.\n",
        "        v = self.value(x)  # (B, T, head_size)\n",
        "\n",
        "        # Weighted sum of values using attention weights.\n",
        "        #\n",
        "        # For each batch b:\n",
        "        #   wei[b] is (T, T) and v[b] is (T, head_size)\n",
        "        #   out[b] = wei[b] @ v[b] gives (T, head_size)\n",
        "        #\n",
        "        # Interpretation:\n",
        "        #   out[b, t, :] = sum_{i=0..T-1} wei[b, t, i] * v[b, i, :]\n",
        "        # and because of the mask, this is effectively sum_{i<=t}.\n",
        "        out = wei @ v  # (B, T, head_size)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79acd7e73015cc59",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head causal self-attention.\n",
        "\n",
        "    This module runs several independent causal self-attention \"heads\" in parallel,\n",
        "    concatenates their outputs, and then projects the result back to the model\n",
        "    embedding dimension (n_embed).\n",
        "\n",
        "    Why multiple heads?\n",
        "      - Each head attends to the sequence in a different subspace.\n",
        "      - Different heads can specialize (syntax, long-range deps, local patterns, etc.).\n",
        "      - Concatenation preserves all head-specific information before mixing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads: int, head_size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create `num_heads` independent attention heads.\n",
        "        #\n",
        "        # Each Head:\n",
        "        #   input:  (B, T, n_embed)\n",
        "        #   output: (B, T, head_size)\n",
        "        #\n",
        "        # ModuleList is required so PyTorch:\n",
        "        #   - registers the submodules\n",
        "        #   - tracks their parameters\n",
        "        #   - moves them with .to(device)\n",
        "        self.heads = nn.ModuleList(\n",
        "            [Head(head_size) for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "        # Final linear projection that mixes information from all heads.\n",
        "        #\n",
        "        # Input dimension:  head_size * num_heads\n",
        "        # Output dimension: n_embed\n",
        "        #\n",
        "        # This allows the model to:\n",
        "        #   - recombine features from different heads\n",
        "        #   - return to the standard embedding width expected by later layers\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embed)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "          x: (B, T, n_embed)\n",
        "             Token representations with positional information already added.\n",
        "\n",
        "        Returns:\n",
        "          out: (B, T, n_embed)\n",
        "               Context-enriched representations after multi-head attention.\n",
        "        \"\"\"\n",
        "\n",
        "        # Run all attention heads in parallel on the same input x.\n",
        "        #\n",
        "        # For each head h:\n",
        "        #   h(x) has shape (B, T, head_size)\n",
        "        #\n",
        "        # The list comprehension produces a list of tensors:\n",
        "        #   [ (B,T,head_size), (B,T,head_size), ..., num_heads times ]\n",
        "        #\n",
        "        # torch.cat(..., dim=-1) concatenates along the channel dimension:\n",
        "        #   (B, T, head_size * num_heads)\n",
        "        #\n",
        "        # This preserves all head outputs side-by-side.\n",
        "        out = torch.cat(\n",
        "            [h(x) for h in self.heads],\n",
        "            dim=-1\n",
        "        )  # (B, T, head_size * num_heads)\n",
        "\n",
        "        # Linearly project concatenated head outputs back to n_embed.\n",
        "        #\n",
        "        # This step:\n",
        "        #   - mixes information across heads\n",
        "        #   - restores the model's canonical embedding width\n",
        "        #\n",
        "        # Without this projection, downstream layers would need to handle\n",
        "        # a wider tensor, and heads would never interact.\n",
        "        out = self.proj(out)  # (B, T, n_embed)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c871de182fbde17d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise Feed-Forward Network (FFN).\n",
        "\n",
        "    PURPOSE\n",
        "    -------\n",
        "    The FeedForward block applies a non-linear transformation to each token\n",
        "    *independently* after attention has mixed information across tokens.\n",
        "\n",
        "    Conceptually:\n",
        "      - Self-attention = \"communication\" between tokens\n",
        "      - FeedForward    = \"computation\" within each token\n",
        "\n",
        "    This block allows the model to:\n",
        "      - increase representational capacity\n",
        "      - apply non-linear feature transformations\n",
        "      - re-encode attended information in a richer way\n",
        "\n",
        "    Crucially:\n",
        "      - It does NOT mix information across time steps.\n",
        "      - Each token position is processed independently and identically.\n",
        "      - The same MLP is shared across all positions.\n",
        "\n",
        "    INPUT / OUTPUT SHAPES\n",
        "    ---------------------\n",
        "      Input:  x of shape (B, T, n_embed)\n",
        "      Output: x of shape (B, T, n_embed)\n",
        "\n",
        "    where:\n",
        "      B = batch size\n",
        "      T = sequence length\n",
        "      n_embed = model embedding dimension\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embed: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # The feed-forward network is a simple 2-layer MLP:\n",
        "        #\n",
        "        #   n_embed  -> 4*n_embed -> n_embed\n",
        "        #\n",
        "        # The expansion factor (4x) is standard in Transformer architectures.\n",
        "        # It gives the model a wider intermediate space to learn complex\n",
        "        # feature interactions before projecting back to the model dimension.\n",
        "        #\n",
        "        # This entire network is applied independently to each token.\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),  # expand feature dimension\n",
        "            nn.ReLU(),                        # non-linearity\n",
        "            nn.Linear(4 * n_embed, n_embed),  # project back to model dimension\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "          x: (B, T, n_embed)\n",
        "             Token representations after attention and (usually) layer normalization.\n",
        "\n",
        "        Returns:\n",
        "          out: (B, T, n_embed)\n",
        "               Transformed token representations.\n",
        "\n",
        "        Note:\n",
        "          - The same MLP is applied to every token position.\n",
        "          - There is no interaction between tokens here.\n",
        "        \"\"\"\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c181b58cd4ea3f48",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer block: multi-head causal self-attention + feed-forward MLP,\n",
        "    with residual connections.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embed: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        assert n_embed % num_heads == 0\n",
        "        head_size = n_embed // num_heads\n",
        "\n",
        "        self.sa = MultiHeadAttention(num_heads=num_heads, head_size=head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "\n",
        "        # Recommended later (stability):\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Simple residual version (works, not as stable as LayerNorm version):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "5d185445a240e69c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:34:58.432664Z",
          "start_time": "2025-12-27T19:34:58.422094Z"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "class BigramLanguageModel4(nn.Module):\n",
        "    def __init__(self, num_heads: int = 4, n_layers: int = 3):\n",
        "        super().__init__()\n",
        "        assert n_embed % num_heads == 0, \"n_embed must be divisible by num_heads\"\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Block(n_embed=n_embed, num_heads=4)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)              # (B,T,n_embed)\n",
        "        pos = torch.arange(T, device=idx.device)               # (T,)\n",
        "        pos_emb = self.position_embedding_table(pos)           # (T,n_embed)\n",
        "        x = tok_emb + pos_emb                                  # (B,T,n_embed)\n",
        "\n",
        "        x = self.blocks(x)                                     # (B,T,n_embed)\n",
        "        x = self.ln_f(x)                                       # (B,T,n_embed)\n",
        "        logits = self.lm_head(x)                               # (B,T,vocab)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.reshape(B*T, logits.size(-1)),\n",
        "                targets.reshape(B*T),\n",
        "            )\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature: float = 1.0, top_k: int | None = None):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -block_size:]\n",
        "                logits, _ = self(idx_cond)\n",
        "                last_logits = logits[:, -1, :]\n",
        "\n",
        "                if temperature != 1.0:\n",
        "                    last_logits = last_logits / temperature\n",
        "\n",
        "                if top_k is not None:\n",
        "                    v, _ = torch.topk(last_logits, k=top_k, dim=-1)\n",
        "                    cutoff = v[:, -1].unsqueeze(-1)\n",
        "                    last_logits = last_logits.masked_fill(last_logits < cutoff, float(\"-inf\"))\n",
        "\n",
        "                probs = F.softmax(last_logits, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "                idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        # Optional: don't force train mode here; let caller control it.\n",
        "        # self.train()\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "df262895944de15e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:49:00.925169Z",
          "start_time": "2025-12-27T19:35:08.441161Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9423200488090515\n"
          ]
        }
      ],
      "source": [
        "m = BigramLanguageModel4().to(device)\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "batch_size = 32\n",
        "for _steps in range(10000):\n",
        "    xb, yb = get_batch('train')\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "#Ran in 13m 52s\n",
        "#loss=0.9423200488090515"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "5983f587559f18dc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-12-27T19:54:29.112580Z",
          "start_time": "2025-12-27T19:54:25.823570Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "To find me thy greatest friends that must seem,\n",
            "The raren of the souls had not the heart to say\n",
            "Yet thus exquisite: 'twere as the mad joy\n",
            "That balling to the higher-degree;\n",
            "But that thou with bright forwardly slain:\n",
            "High as we will devotion his chince;\n",
            "How he could make a lisping, without-book;\n",
            "The world doth each one first of the name\n",
            "To have look what seems unsuspicious and\n",
            "When in the which shame to win our fox?\n",
            "\n",
            "AEdile:\n",
            "It is not.\n",
            "\n",
            "AEdile:\n",
            "Do not yet, with these few wonder wonder on thee.\n",
            "\n",
            "GREGORY:\n",
            "No, masters, my lord-spearers.\n",
            "\n",
            "SAMPSON:\n",
            "If; but we thrive now the benefits not\n",
            "speak against help; but to be sworth in law,\n",
            "Whether receive be not stoop'd by the wind.\n",
            "\n",
            "GREGORY:\n",
            "They say, bethink me, but not prosper in the guilty\n",
            "Than I am coat, this right way but drown;\n",
            "And hastely dived as incle York it did.\n",
            "\n",
            "KING HENRY VI:\n",
            "You slew in his charge; bulk as never wear\n",
            "Thus it makes us the wealth upon this clothes?\n",
            "And so well I; and this blood upon this eyes,\n",
            "They were a but sutter'd in\n"
          ]
        }
      ],
      "source": [
        "start = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(start, max_new_tokens=1000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
